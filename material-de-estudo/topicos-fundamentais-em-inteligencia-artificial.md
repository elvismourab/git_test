# T√≥picos Fundamentais em Intelig√™ncia Artificial

## üéØ Objetivo

Este material de estudo apresenta uma vis√£o geral de t√≥picos essenciais em Intelig√™ncia Artificial, incluindo a engenharia de prompts para otimizar as respostas de modelos de linguagem, as t√©cnicas de fine-tuning para especializa√ß√£o de modelos e o conceito de Small Language Models (SLMs).

## üìù Resumo

A otimiza√ß√£o de modelos de IA envolve tanto a cria√ß√£o de prompts eficazes (Prompt Engineering) quanto o ajuste fino (fine-tuning) com dados espec√≠ficos. O fine-tuning pode ser supervisionado (SFT), para ensinar respostas corretas, ou baseado em prefer√™ncia humana (DPO), para alinhar o modelo com o que os usu√°rios consideram de alta qualidade. Al√©m dos grandes modelos (LLMs), os Small Language Models (SLMs) oferecem uma alternativa especializada e focada em dom√≠nios espec√≠ficos.

## üìö Conte√∫do

### 1. Prompt Engineering

**Prompt Engineering** √© a arte e a ci√™ncia de criar *prompts* (comandos) de forma otimizada para obter as melhores respostas de um modelo de linguagem. As principais estrat√©gias incluem:

*   **Escrever instru√ß√µes claras:**
    *   Incluir detalhes espec√≠ficos na sua consulta.
    *   Pedir ao modelo para adotar uma **persona** (ex: "Aja como um professor de hist√≥ria...").
    *   Usar delimitadores (como `---`, `"""` ou `<exemplo>`) para separar partes do *input*.
    *   Especificar os passos necess√°rios para a conclus√£o de uma tarefa.
    *   Fornecer exemplos para guiar o modelo.
    *   Especificar o tamanho desejado para a sa√≠da.
    *   Fornecer um texto de refer√™ncia e instruir o modelo a responder com cita√ß√µes desse texto.
*   **Dividir tarefas complexas:**
    *   Usar a **classifica√ß√£o de inten√ß√µes** para identificar a instru√ß√£o mais relevante para uma consulta.
    *   Em di√°logos longos, resumir ou filtrar conversas anteriores.
    *   Resumir documentos grandes em partes e construir um resumo final recursivamente.
*   **Dar tempo para o modelo "pensar":**
    *   Instruir o modelo a resolver o problema internamente antes de dar a resposta final.
    *   Usar "mon√≥logo interno" para ocultar o processo de racioc√≠nio.
    *   Perguntar ao modelo se ele esqueceu de algo em etapas anteriores.
*   **Usar ferramentas externas:**
    *   Utilizar **pesquisa baseada em *embeddings*** para recuperar conhecimento de forma eficiente.
    *   Executar c√≥digo para realizar c√°lculos precisos ou chamar APIs.
    *   Dar acesso do modelo a fun√ß√µes espec√≠ficas.
*   **Testar sistematicamente:**
    *   Avaliar as sa√≠das do modelo comparando-as com respostas de refer√™ncia ("gold-standard").

### 2. Fine-tuning e Otimiza√ß√£o de Modelos

#### 2.1 Conceitos e Par√¢metros de Treinamento

O **fine-tuning** √© o processo de ajustar um modelo pr√©-treinado com um conjunto de dados espec√≠fico para uma tarefa particular. Para isso, alguns par√¢metros s√£o cruciais:

*   **Batch size:** Define quantos exemplos de treinamento o modelo processa por vez antes de atualizar seus pesos. O termo "epoch" pode ser dividido em v√°rios "batches".
*   **Learning rate multiplier:** Um valor entre 1 e 10 que afeta a velocidade do aprendizado. Um valor baixo torna o aprendizado mais detalhista e lento, enquanto um valor alto pode fazer o modelo "pular" detalhes e n√£o aprender corretamente.
*   **Number of epochs:** √â o n√∫mero de vezes que o modelo percorre todo o conjunto de dados de treinamento.

**Dados de Treinamento:**
*   O arquivo JSON deve seguir o formato da documenta√ß√£o.
*   √â crucial ter um arquivo com **m√©tricas de valida√ß√£o** para avaliar o modelo.
*   Cada exemplo no conjunto de dados deve ser formatado como uma conversa, com uma lista de mensagens (cada uma com `role`, `content` e opcionalmente `name`).
*   Dados de alta qualidade s√£o mais importantes que uma grande quantidade de dados de baixa qualidade. Dobrar o n√∫mero de exemplos de treinamento tende a gerar um aumento similar na qualidade do modelo.

#### 2.2 Fine-tuning Supervisionado (SFT) vs. Direct Preference Optimization (DPO)

| Caracter√≠stica | Fine-tuning Supervisionado (SFT) | Direct Preference Optimization (DPO) |
| :--- | :--- | :--- |
| **O que √©?** | Ajuste de modelo com exemplos rotulados com **respostas corretas**. | Ajuste de modelo com base em **prefer√™ncias humanas** (pares de respostas, uma preferida e outra n√£o preferida). |
| **Como funciona?** | O modelo aprende a imitar a resposta ideal. | O modelo aprende a aumentar a probabilidade de gerar a resposta preferida. |
| **Objetivo** | Gerar respostas **certas** em tarefas espec√≠ficas (ex: tradu√ß√£o, QA). | Gerar respostas que **agradam mais os humanos**, focando em qualidade e alinhamento √©tico. |

#### 2.3 A Escolha da "Seed" Rand√¥mica

A **seed** √© um n√∫mero usado para inicializar um gerador de n√∫meros aleat√≥rios, garantindo que experimentos reproduzam os mesmos resultados. O valor 42 √© frequentemente usado como um padr√£o em ci√™ncia de dados e programa√ß√£o.

*   **Motivo:** A escolha √© uma refer√™ncia cultural ao livro "O Guia do Mochileiro das Gal√°xias", de Douglas Adams, onde 42 √© a "resposta para a Vida, o Universo e Tudo Mais". √â uma piada interna que se tornou um padr√£o de comunidade.

### 3. Small Language Models (SLMs)

Enquanto os LLMs (Large Language Models) s√£o treinados em vastas quantidades de dados da internet, os **SLMs** (Small Language Models) s√£o ajustados em **conjuntos de dados menores e especializados**.

*   **Vantagem:** Um SLM pode ser treinado especificamente para um setor ou tarefa, como a √°rea da sa√∫de. Por exemplo, um chatbot pode ser treinado apenas com dados m√©dicos para dar respostas mais precisas e relevantes a perguntas sobre sa√∫de, sem a necessidade de processar dados irrelevantes como poemas ou romances.
*   **Benef√≠cio:** Foco e especializa√ß√£o em um dom√≠nio espec√≠fico, garantindo qualidade e precis√£o para casos de uso pontuais.